name: "sac_state"

agent:
  _partial_: true
  _target_: segdac.agents.sac.agent.SacAgent
  action_sampling_strategy:
    _target_: segdac.agents.action_sampling_strategy.StochasticActionSamplingStrategy
    actor:
      _target_: segdac.agents.sac.actor.SacActor
      network:
        _target_: segdac.networks.network_wrapper.ActorTensorDictNetworkWrapper
        network:
          _target_: segdac.networks.mlp.Mlp
          in_features: 0 # This is set automatically in code
          hidden_depth: 3
          input_norm_class:
            _partial_: true
            _target_: torch.nn.LayerNorm
          hidden_neurons: 256
          hidden_norm_class:
            _partial_: true
            _target_: torch.nn.Identity
          hidden_activation_class:
            _partial_: true
            _target_: torch.nn.ReLU
          output_activation_class:
            _partial_: true
            _target_: torch.nn.Identity # A Tanh is automatically applied from the distribution object so we use Identity here
          out_features: 0 # This is set automatically in code
        in_keys:
          - "state"
      policy_optimizer:
        _partial_: true
        _target_: torch.optim.Adam
        lr: 3e-4
      entropy_optimizer:
        _partial_: true
        _target_: torch.optim.Adam
        lr: 3e-4
      device: ${policy_device}
      action_dim: 0 # This is set automatically in code
      distribution_factory: ${algo.distribution_factory}
      initial_entropy: 1.0
      max_grad_norm: null
    distribution_factory: ${algo.distribution_factory}
  critic:
    _target_: segdac.agents.sac.critic.SacCritic
    target_params_updater: ${algo.target_params_updater}
    gamma: 0.8
    q_function_1: ${algo.q_function}
    q_function_2: ${algo.q_function}
    q_function_loss:
      _target_: torch.nn.MSELoss
    q_function_optimizer:
      _partial_: true
      _target_: torch.optim.Adam
      lr: 3e-3
    distribution_factory: ${algo.distribution_factory}
    device: ${policy_device}
    max_grad_norm: null
  critic_update_frequency: 1
  actor_update_frequency: 1
  target_networks_update_frequency: 2

distribution_factory:
  _target_: segdac.agents.distribution_factory.SquashedNormalFactory
  min_logstd: -10
  max_logstd: 2

q_function:
  _target_: segdac.networks.network_wrapper.CriticTensorDictNetworkWrapper
  network:
    _target_: segdac.networks.mlp.Mlp
    in_features: 0 # This is set automatically in code based on the environment
    hidden_depth: 3
    input_norm_class:
      _partial_: true
      _target_: torch.nn.Identity
    hidden_neurons: 256
    hidden_norm_class:
      _partial_: true
      _target_: torch.nn.Identity
    hidden_activation_class:
      _partial_: true
      _target_: torch.nn.ReLU
    output_activation_class:
      _partial_: true
      _target_: torch.nn.Identity
    out_features: 1
  in_keys:
    - "state"
    - "action"

target_params_updater:
  _target_: segdac.agents.target_networks_params_updaters.polyak_average_updater.PolyakAverageParametersUpdater
  tau: 0.01

config_updater:
  _target_: segdac_dev.config.sac_config_updaters.SacStateConfigUpdater

env:
  train_transforms:
    - _target_: segdac_dev.envs.transforms.traj_ids.TrajIdsTransform
      device: ${policy_device}
      num_envs: ${training.env_config.num_envs}
  eval_transforms:
    - _target_: segdac_dev.envs.transforms.traj_ids.TrajIdsTransform
      device: ${policy_device}
      num_envs: ${evaluation.env_config.num_envs}

replay_buffer:
  capacity: 1_000_000
  keys_to_exclude: ["pixels"]
  save_transforms: []
  sample_transforms: []
